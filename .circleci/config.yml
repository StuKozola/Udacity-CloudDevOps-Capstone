# Configuration file for MLFLOW Deployment to AWS K8s
version: 2.1
orbs:
  anchore: anchore/anchore-engine@1.8.6

parameters:
  memstash-token:
    type: string
    default: "3ffae65a-7ef5-43ba-be64-8cb88a18dd11"

commands:
  destroy-environment:
    description: Destroy s3 bucket and EKS infrastructure for current workflow
    steps:
      - run:
          name: Destroy environments
          when: on_fail
          command: |
            aws s3 rm --recursive s3://mlflow-${CIRCLE_WORKFLOW_ID:0:7}
            aws cloudformation delete-stack --stack-name mlflow-artifacts-${CIRCLE_WORKFLOW_ID:0:7}
            eksctl delete cluster --name mlflow-eks-${CIRCLE_WORKFLOW_ID:0:7} --region ${AWS_DEFAULT_REGION}

  test-add-experiments:
    description:  Test MLFlow tracking server by adding experiments and verifying added
    steps:
    - run:
        name: Add experiments to repository and verify
        command: |
          source .env
          echo "MLFLOW SERVER: ${MLFLOW_SERVER_HOSTNAME}"
          base_url=http://${MLFLOW_SERVER_HOSTNAME}:5000/api/2.0/preview/mlflow/experiments
          # list experiments
          curl ${base_url}/list
          # add experiment
          EXP_ID=$(
            curl -H "Content-Type:application/json" -X POST \
              --data '{"name": "test_experiment"}' ${base_url}/create | jq '.experiment_id'
            )
            echo "Experiment ID: $EXP_ID"
            curl ${base_url}/list
            # retrieve experiment to verify
            EXP_NAME=$(
              curl -H "Content-Type:application/json" -X GET --data '{"experiment_id": '"$EXP_ID"' }' ${base_url}/get | jq '.experiment.name'
            )
            if [[ "$EXP_NAME" == '"test_experiment"' ]]
            then
              echo "Successfully added and retrieved experiment"
            else
              echo "failure"
              echo $EXP_NAME
              exit 1
            fi

jobs:
  build-server:
    docker:
      - image: python:3.8.2-slim
    executor: anchore/anchore_engine
    working_directory: ~/repo
    
    steps:
      - checkout

      - run:
          name: Install dependencies
          command: |
            circleci-agent step halt
            apt -y update
            apt -y upgrade
            apt -y install sudo
            ls -al /bin/sh && sudo rm /bin/sh && sudo ln -s /bin/bash /bin/sh && ls -al /bin/sh
            apt -y install git
            apt -y install wget
            apt -y install make
           
      - run:
          name: Install Hadolint
          command: make install-hadolint
      
      - run:
          name: Install Docker client
          command: make install-docker-cli
           
      - run:
          name: Linting Dockerfile
          command: |
            make lint
      
      - setup_remote_docker:
          version: 20.10.2
          docker_layer_caching: true

      - run:
          name: Build Image
          command: |
            docker build -t mlflow_server .
            docker image ls
      
      - anchore/analyze_local_image:
          image_name: 'mlflow_server:latest'
          policy_failure: false
          timeout: '500'

      - anchore/parse_reports
      
      - store_artifacts:
          path: anchore-reports
      
      - run:
          name: Publishing MLFLOW Container
          command: |
            echo $DOCKER_PASS | docker login -u $DOCKER_USER --password-stdin
            docker image tag mlflow_server $DOCKER_USER/mlflow_server
            docker push $DOCKER_USER/mlflow_server

  test-server:
    machine: true
    working_directory: ~/repo
    steps:
      - checkout
      - run: 
          name: Download image and check that is it running
          command: |
            circleci-agent step halt
            docker pull $DOCKER_USER/mlflow_server
            docker run -p 5000:5000 -d --name test_server $DOCKER_USER/mlflow_server --host 0.0.0.0
            docker container ls
            sleep 10
            if [[ `curl -s -o /dev/null -w "%{http_code}" http://localhost:5000` == 200 ]] 
            then
              echo "Successfully connected to running docker image"
            else
              echo "failure"
              exit 1
            fi
            
            echo "MLFLOW_SERVER_HOSTNAME=localhost" > .env

      - test-add-experiments
  
  deploy-infrastructure:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            yum -y install tar gzip jq git
            yum -y install sudo
            ls -al /bin/sh && sudo rm /bin/sh && sudo ln -s /bin/bash /bin/sh && ls -al /bin/sh
           
      - run:
          name: Install kubectl and eksctl
          command: |
            curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/linux/amd64/kubectl
            chmod +x ./kubectl
            sudo mv ./kubectl /usr/local/bin
            kubectl version --short --client
            curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
            sudo mv /tmp/eksctl /usr/local/bin
            eksctl version
      
      - run:
          name: Ensure artifact repository exists (S3 bucket)
          command: |
            aws cloudformation deploy \
              --template-file .circleci/aws/artifact-store.yml \
              --tags project=mlflow \
              --stack-name "mlflow-artifacts-${CIRCLE_WORKFLOW_ID:0:7}" \
              --parameter-overrides WorkflowID=${CIRCLE_WORKFLOW_ID:0:7}
      
      - run:
          name: Create K8 Infrastructure on AWS
          command: |
            eksctl create cluster \
              --name "mlflow-eks-${CIRCLE_WORKFLOW_ID:0:7}" \
              --region ${AWS_DEFAULT_REGION} \
              --with-oidc \
              --ssh-access \
              --ssh-public-key ${AWS_EC2_KEY_NAME} \
              --managed
            kubectl version --short --client
            kubectl get po -A
      
      - run:
          name: Create PostgreSQL and MLFlow Services
          command: |
            # set environment variables for k8s
            export MLFLOW_S3_ENDPOINT_URL="S3://mlflow-${CIRCLE_WORKFLOW_ID:0:7}"
            echo "MLFLOW_S3_ENDPOINT_URL set to $MLFLOW_S3_ENDPOINT_URL"

            # add envs to env-config.yml
            cat \<< EoF >> ./k8/env-config.yml
            data:
              MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}
              AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
              AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
            EoF
            cat ./k8/env-config.yml

            # deploy to k8s
            kubectl create namespace mlflow
            kubectl apply -f k8/env-config.yml
            kubectl create -f k8/postgres.yml
            kubectl create -f k8/mlflow-server.yml
            kubectl get services
            kubectl get nodes -o wide
            kubectl get pods --all-namespaces -o wide

      - run:
          name: Assign IAM policy for S3 Access
          command: |
            for STACK_NAME in $(eksctl get nodegroup --cluster "mlflow-eks-${CIRCLE_WORKFLOW_ID:0:7}" -o json | jq -r '.[].StackName')
            do
              ROLE_NAME=$(aws cloudformation describe-stack-resources --stack-name $STACK_NAME | jq -r '.StackResources[] | select(.ResourceType=="AWS::IAM::Role") | .PhysicalResourceId')

            aws iam attach-role-policy \
              --role-name $ROLE_NAME \
              --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
            done
      
      - run:
          name: Expose service and verify it is available
          command: |
            kubectl -n mlflow get svc mlflow-service
            kubectl -n mlflow patch svc mlflow-service -p '{"spec": {"type":"LoadBalancer"}}'
            sleep 10
            kubectl -n mlflow get svc mlflow-service
            export MLFLOW_SERVER_HOSTNAME=(`kubectl -n mlflow get svc mlflow-service -o=jsonpath="{.status.loadBalancer.ingress[].hostname}"`)
            
            # put HOSTNAME in the shared memory
            curl -H "Content-Type: text/plain" \
              -H "token: << pipeline.parameters.memstash-token >>" \
              --request PUT \
              --data "$MLFLOW_SERVER_HOSTNAME" https://api.memstash.io/values/MLFLOW_SERVER_HOSTNAME_${CIRCLE_WORKFLOW_ID:0:7}
            echo "MLFLOW SERVER is at ${MLFLOW_SERVER_HOSTNAME}"
            echo "MLFLOW_SERVER_HOSTNAME=${MLFLOW_SERVER_HOSTNAME}" > .env

            # wait 5 minutes for pods to start up and servers to be available
            sleep 300
            curl http://${MLFLOW_SERVER_HOSTNAME}:5000/api/2.0/preview/mlflow/experiments/list

      - test-add-experiments
      #- destroy-environment

  smoke-test:
    docker:
      - image: python:3.8.2-slim
    working_directory: ~/repo
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            apt -y update
            apt -y upgrade
            apt -y install sudo
            ls -al /bin/sh && sudo rm /bin/sh && sudo ln -s /bin/bash /bin/sh && ls -al /bin/sh
            apt -y install git
            apt -y install wget
            apt -y install make
            apt -y install curl
            apt -y install jq

      - run:
          name: Set up python environment and run test
          command: |
            MLFLOW_SERVER_HOSTNAME=`curl -H \
              "token: << pipeline.parameters.memstash-token >>" \
              --request GET https://api.memstash.io/values/MLFLOW_SERVER_HOSTNAME_${CIRCLE_WORKFLOW_ID:0:7}`
            export MLFLOW_TRACKING_URI="http://${MLFLOW_SERVER_HOSTNAME}:5000"
            echo "${MLFLOW_TRACKING_URI}"
      
            # number of experiments currently
            base_url=http://${MLFLOW_SERVER_HOSTNAME}:5000/api/2.0/preview/mlflow/experiments
           
            # list experiments
            curl ${base_url}/list
            
            # retrieve experiment number
            EXP_ID_PRIOR=$(
              curl ${base_url}/list | jq '.experiments | last | .experiment_id'
            )
           
            # install python libraries and run experiment
            make smoke-test
            
            # verify 4 experiments added
            EXP_ID=$(
              curl ${base_url}/list | jq '.experiments | last | .experiment_id'
            )

            if [[ $EXP_ID >= (expr $EXP_ID_PRIOR + 4) ]]
            then
              echo "Successfully added and retrieved experiment"
            else
              echo "failure"
              echo $EXP_ID
              exit 1
            fi


workflows:
  MLFlow-AWS:
    jobs:
      - build-server:
          context: CAPSTONE
      - test-server:
          context: CAPSTONE
          requires: [build-server]
      - deploy-infrastructure:
          context: CAPSTONE
          requires: [test-server]
      # - configure-infrastructure:
      #     context: CAPSTONE
      #     requires: [deploy-infrastructure]
      - smoke-test:
          context: CAPSTONE
          requires: [deploy-infrastructure]
      # - cloudfront-update:
      #     context: mlflow
      #     requires: [smoke-test]
      # - cleanup:
      #     context: mlflow
      #     requires: [cloudfront-update]
